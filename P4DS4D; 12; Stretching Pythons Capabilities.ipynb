{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining applications for data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/developers/<BR>\n",
    "http://scikit-learn.org/stable/faq.html<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X, y = boston.data,boston.target\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.73100000e-02   0.00000000e+00   7.07000000e+00   0.00000000e+00\n",
      "   4.69000000e-01   6.42100000e+00   7.89000000e+01   4.96710000e+00\n",
      "   2.00000000e+00   2.42000000e+02   1.78000000e+01   3.96900000e+02\n",
      "   9.14000000e+00] 24.0\n"
     ]
    }
   ],
   "source": [
    "print X[1], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "hypothesis = LinearRegression(normalize=True)\n",
    "hypothesis.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00\n",
      "  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00\n",
      "   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03\n",
      "  -5.25466633e-01]\n"
     ]
    }
   ],
   "source": [
    "print hypothesis.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print len(hypothesis.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.740607742865\n"
     ]
    }
   ],
   "source": [
    "print hypothesis.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "[ 25.8972784]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbajaj/Library/Python/2.7/lib/python/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "new_observation = np.array([1,0,1,0,0.5,7,59,6,3,200,20,350,4],dtype=float)\n",
    "print len(new_observation)\n",
    "print hypothesis.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74060774286494313"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#help(LinearRegression)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X)\n",
    "print scaler.transform(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing the Hashing Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using hash functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2359742753373747800\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "print hash('Python')\n",
    "print abs(hash('Python')) % 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating the hashing trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "string_1 = 'Python for data science'\n",
    "string_2 = 'Python for machine learning'\n",
    "\n",
    "def hashing_trick(input_string, vector_size=20):\n",
    "    feature_vector = [0] * vector_size\n",
    "    for word in input_string.split(' '):\n",
    "        index = abs(hash(word)) % vector_size\n",
    "        feature_vector[index] = 1\n",
    "    return feature_vector\n",
    "\n",
    "print hashing_trick(input_string='Python for data science', vector_size=20)\n",
    "print hashing_trick(input_string='Python for machine learning', vector_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 18)\t1\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "print csc_matrix([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x20 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "sklearn_hashing_trick = HashingVectorizer(n_features=20, binary=True, norm=None)\n",
    "hashed_text = sklearn_hashing_trick.transform(['Python for data science','Python for machine learning'])\n",
    "hashed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "one_hot_enconder = CountVectorizer()\n",
    "one_hot_enconded = one_hot_enconder.fit_transform(['Python for data science','Python for machine learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'machine': 3, u'learning': 2, u'for': 1, u'python': 4, u'science': 5, u'data': 0}\n"
     ]
    }
   ],
   "source": [
    "print one_hot_enconder.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x20 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_hashing_trick.transform(['New text has arrived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x4 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_enconder.fit_transform(['New text has arrived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HashingVectorizer is the perfect function to use when your data can’t fit\n",
    "into memory and its features aren’t fixed. In the other cases, consider using\n",
    "the more intuitive CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 56.3 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit l = [k for k in range(10**6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 loops, best of 5: 55.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 20 -r 5 l = [k for k in range(10**6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 101 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "l = list()\n",
    "for k in range(10**6):\n",
    "    l.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "sklearn_hashing_trick = HashingVectorizer(n_features=20, binary=True, norm=None) \n",
    "one_hot_enconder = CountVectorizer()\n",
    "texts = ['Python for data science','Python for machine learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 1.05 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit one_hot_enconded = one_hot_enconder.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 115 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit  hashing = sklearn_hashing_trick.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000127188515663\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "cumulative_time = timeit.timeit(\"hashing = sklearn_hashing_trick.transform(texts)\", \n",
    "                                 \"from __main__ import sklearn_hashing_trick, texts\", \n",
    "                                 number=10000)\n",
    "print cumulative_time / 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-9f1b822f66e5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-9f1b822f66e5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    pip install psutil\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Installation procedures from the command line:\n",
    "# pip install psutil\n",
    "# pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "# Initialization from IPython (to be repeat at every IPython start)\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 99.32 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "hashing = sklearn_hashing_trick.transform(texts)\n",
    "%memit dense_hashing = hashing.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example_code.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile example_code.py\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "def comparison_test():\n",
    "    sklearn_hashing_trick = HashingVectorizer(n_features=20, binary=True, norm=None) \n",
    "    one_hot_enconder = CountVectorizer()\n",
    "    texts = ['Python for data science','Python for machine learning']\n",
    "    one_hot_enconded = one_hot_enconder.fit_transform(texts)\n",
    "    hashing = sklearn_hashing_trick.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('',)\n"
     ]
    }
   ],
   "source": [
    "from example_code import comparison_test\n",
    "%mprun -f comparison_test comparison_test()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Filename: example_code.py\n",
    "\n",
    "Line #    Mem usage    Increment   Line Contents\n",
    "================================================\n",
    "     2     99.3 MiB      0.0 MiB   def comparison_test():\n",
    "     3     99.3 MiB      0.0 MiB       sklearn_hashing_trick = HashingVectorizer(n_features=20, binary=True, norm=None) \n",
    "     4     99.3 MiB      0.0 MiB       one_hot_enconder = CountVectorizer()\n",
    "     5     99.3 MiB      0.0 MiB       texts = ['Python for data science','Python for machine learning']\n",
    "     6     99.3 MiB      0.0 MiB       one_hot_enconded = one_hot_enconder.fit_transform(texts)\n",
    "     7     99.3 MiB      0.0 MiB       hashing = sklearn_hashing_trick.transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating multiprocessing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X, y = digits.data,digits.target\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 11.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit single_core_learning = cross_val_score(SVC(), X, y, cv=20, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 4.05 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit multi_core_learning = cross_val_score(SVC(), X, y, cv=20, n_jobs=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
